---
title: "Google AI (Gemini)"
description: "Use Gemini models from Google in your BoostGPT agents"
---

## Overview

Google's Gemini models offer industry-leading context windows (up to 2 million tokens), exceptional multimodal capabilities, and strong performance across text, code, and reasoning tasks.

## Available Models

### Gemini 3 Pro Preview (Latest)

<Card title="Gemini 3 Pro Preview" icon="sparkles">
  **5 credits** • Cutting-edge preview model

  - 2,000,000 token context window (largest available)
  - Exceptional reasoning and contextual understanding
  - Speed: Medium • Cost: High
  - Best for: Research, massive documents, advanced R&D
</Card>

### Gemini 2.5 Series (Stable)

<CardGroup cols={2}>
  <Card title="Gemini 2.5 Pro" icon="crown">
    **3 credits** • Most advanced stable model

    - 2,000,000 token context window
    - Exceptional reasoning and accuracy
    - Speed: Slow • Cost: High
    - Best for: Complex reasoning, long documents
  </Card>

  <Card title="Gemini 2.5 Flash" icon="bolt">
    **2 credits** • Fast and capable

    - 1,000,000 token context window
    - Excellent reasoning with speed
    - Speed: Fast • Cost: Medium
    - Best for: Production applications
  </Card>

  <Card title="Gemini 2.5 Flash Lite" icon="zap">
    **1 credit** • Ultra-fast and efficient

    - 1,000,000 token context window
    - Good reasoning at lowest cost
    - Speed: Very Fast • Cost: Very Low
    - Best for: High-volume, simple tasks
  </Card>

  <Card title="Gemini 2.0 Flash Thinking" icon="brain">
    **3 credits** • Reasoning model

    - 1,000,000 token context window
    - Explicit thinking process for analysis
    - Speed: Medium • Cost: Medium
    - **Reasoning model** for problem-solving
  </Card>
</CardGroup>

## Setup

### Using BoostGPT-Hosted API Keys

<Steps>
  <Step title="Select Gemini Model">
    In your BoostGPT dashboard, select any Gemini model when creating or configuring your bot.
  </Step>

  <Step title="Choose Your Model">
    - **Gemini 2.5 Flash**: Best for most production use cases
    - **Gemini 2.5 Pro**: When you need massive 2M context
    - **Gemini 2.5 Flash Lite**: High-volume, cost-sensitive
    - **Gemini 2.0 Flash Thinking**: Complex reasoning tasks
  </Step>
</Steps>

### Using Your Own Google AI API Key

<Tabs>
  <Tab title="Dashboard Setup">
    <Steps>
      <Step title="Navigate to Integrations">
        Go to [app.boostgpt.co](https://app.boostgpt.co/integrations) and select **Integrations**
      </Step>

      <Step title="Select Google AI">
        Find and click on the **Google** provider
      </Step>

      <Step title="Add API Key">
        Get your API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
        
        Enter the API key and select which agents will use it
      </Step>

      <Step title="Save Configuration">
        Click save to apply your custom API key
      </Step>
    </Steps>

    <Tip>
      Google AI offers a generous free tier for testing and development!
    </Tip>
  </Tab>

  <Tab title="Core SDK">
    ```javascript bot.js
    import { BoostGPT } from 'boostgpt';

    const client = new BoostGPT({
      project_id: process.env.BOOSTGPT_PROJECT_ID,
      key: process.env.BOOSTGPT_API_KEY
    });

    // Create bot with Gemini model
    const botResponse = await client.createBot({
      name: 'My Gemini Bot',
      model: 'gemini-2.5-flash',
      instruction: 'You are a helpful assistant.',
      max_reply_tokens: 1000,
      status: 'active'
    });

    if (botResponse.err) {
      console.error('Error:', botResponse.err);
    } else {
      console.log('Bot created:', botResponse.response);
    }
    ```
  </Tab>

  <Tab title="Router SDK">
    ```javascript router.js
    import { Router, TelegramAdapter } from '@boostgpt/router';

    const router = new Router({
      apiKey: process.env.BOOSTGPT_API_KEY,
      projectId: process.env.BOOSTGPT_PROJECT_ID,
      defaultBotId: process.env.BOOSTGPT_BOT_ID, // Gemini bot
      adapters: [
        new TelegramAdapter({
          telegramToken: process.env.TELEGRAM_TOKEN
        })
      ]
    });

    // Router uses Gemini automatically
    router.onMessage(async (message, context) => {
      // Custom logic
      if (message.content === '/info') {
        return 'Powered by Gemini with 1M context!';
      }

      return null; // Gemini handles it
    });

    await router.start();
    ```
  </Tab>
</Tabs>

## Model Selection Guide

<AccordionGroup>
  <Accordion icon="bolt" title="Gemini 2.5 Flash - Production Workhorse">
    **Best for:**
    - Production chatbots and customer support
    - General-purpose applications
    - Fast responses with strong reasoning
    - 1M context for long conversations

    **Sweet spot:** Best balance of speed, cost, and capability

    **Cost:** 2 credits per request
  </Accordion>

  <Accordion icon="crown" title="Gemini 2.5 Pro - Maximum Context">
    **Best for:**
    - Analyzing entire codebases
    - Processing very long documents (books, research papers)
    - Multi-turn conversations with full history
    - Maximum context retention (2M tokens)

    **Standout feature:** Largest context window available

    **Cost:** 3 credits per request
  </Accordion>

  <Accordion icon="zap" title="Gemini 2.5 Flash Lite - High Volume">
    **Best for:**
    - High-volume applications (thousands of requests)
    - Simple queries and responses
    - Cost-sensitive production
    - Quick classifications

    **Cost:** 1 credit per request (most affordable)
  </Accordion>

  <Accordion icon="brain" title="Gemini 2.0 Flash Thinking - Reasoning">
    **Best for:**
    - Mathematical problem solving
    - Code analysis and debugging
    - Multi-step logical reasoning
    - Scientific tasks

    **Note:** Reasoning model with explicit thinking

    **Cost:** 3 credits per request
  </Accordion>

  <Accordion icon="sparkles" title="Gemini 3 Pro Preview - Cutting Edge">
    **Best for:**
    - Research and experimentation
    - Testing next-generation capabilities
    - Maximum context + latest features

    **Note:** Preview model, may have breaking changes

    **Cost:** 5 credits per request
  </Accordion>
</AccordionGroup>


## Troubleshooting

<AccordionGroup>
  <Accordion icon="circle-exclamation" title="Slow responses with Pro">
    **Expected:** Pro prioritizes accuracy over speed

    **Solutions:**
    - Use Flash for faster responses
    - Reduce input length when possible
    - Add loading indicators
  </Accordion>

  <Accordion icon="circle-exclamation" title="Context length errors">
    **Rare:** 1M-2M context handles most cases

    **Solutions:**
    - Use Pro for maximum 2M context
    - Implement message pruning for extreme cases
    - Split very large documents
  </Accordion>

  <Accordion icon="circle-exclamation" title="Higher costs than expected">
    **Cause:** Long contexts consume many tokens

    **Solutions:**
    - Use Flash Lite for simple tasks (1 credit)
    - Implement context pruning
    - Set max_reply_tokens limits
    - Monitor token usage in dashboard
  </Accordion>
</AccordionGroup>


## Next Steps

<CardGroup cols={2}>
  <Card title="Model Comparison" icon="scale" href="/providers/model-comparison">
    Compare all providers
  </Card>

  <Card title="Reasoning Models" icon="brain" href="/providers/reasoning-models">
    Learn about Flash Thinking
  </Card>

  <Card title="Bring Your Own Key" icon="key" href="/providers/bring-your-own-keys">
    Use your Google AI API key
  </Card>

  <Card title="SDK Reference" icon="code" href="/sdk/core/api-reference">
    Full API documentation
  </Card>
</CardGroup>
